{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaturalLanguageProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashikka/NLP-Project/blob/master/textSummarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "funO8pxLHSAL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24e60e9a-bf2a-4e25-d7c9-2f528db12f60"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo6fM1Rwww_d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eghZqX28xGPl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b02ea05a-eff6-4f61-c3ed-2f88e236fe27"
      },
      "source": [
        "import logging\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.WARNING)\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2i2Kx1KzZwG"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/natural-language-processing/github_issues.csv\")\n",
        "# df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxpWNfjb5TO9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "5c0db8ae-fc83-46f1-bd98-e865bbf05fe2"
      },
      "source": [
        "#read in data sample 2M rows (for speed of tutorial)\n",
        "traindf, testdf = train_test_split(pd.read_csv(\"/content/drive/My Drive/natural-language-processing/github_issues.csv\").sample(n=200), \n",
        "                                   test_size=.10)\n",
        "\n",
        "\n",
        "#print out stats about shape of data\n",
        "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
        "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
        "\n",
        "# preview data\n",
        "traindf.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 180 rows 3 columns\n",
            "Test: 20 rows 3 columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_url</th>\n",
              "      <th>issue_title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2980513</th>\n",
              "      <td>\"https://github.com/PHPMailer/PHPMailer/issues/1299\"</td>\n",
              "      <td>possible issue with doing a 7-bit downgrade?</td>\n",
              "      <td>i posted this issue on so: https://stackoverflow.com/questions/47916568/unintended-indents-with-plain-text-email in trying to figure out what the problem was, i theorized that it was related to the character set always defaulting to us-ascii as posted by the so user: https://stackoverflow.com/questions/43022838/phpmailer-altbody-plain-text-version-of-message-has-always-us-ascii-charset basically, i think that this kind of thing... //can we do a 7-bit downgrade? if '8bit' == $bodyencoding and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5238497</th>\n",
              "      <td>\"https://github.com/attic-labs/noms/issues/3322\"</td>\n",
              "      <td>suggestion: replace roadmap in readme with github milestones</td>\n",
              "      <td>i saw that issue 2327 is now closed, which is an item on the roadmap https://github.com/attic-labs/noms roadmap . while the issue is closed, it was not checked off on the roadmap. the static nature of the roadmap makes this an easy oversight. i think github's milestones exists for the exact purpose for which you've created the roadmap and it will never report false status ; my apologies for the pedantic and low priority issues that i've been raising this week!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5230334</th>\n",
              "      <td>\"https://github.com/aantron/bisect_ppx/issues/143\"</td>\n",
              "      <td>factor out bisect_ppx-ocamlbuild as a separate opam package</td>\n",
              "      <td>this is to eliminate the depopt https://github.com/aantron/bisect_ppx/blob/07a994f6bbe8724ad62e77b75f2c326810da33ea/bisect_ppx.opam l33 :scream: we temporarily got as part of 117, and is a follow-on to that pr. we should release once with the depopt, but with the new package, and warn people using the ocamlbuild plugin to switch to it.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    issue_url  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 body\n",
              "2980513  \"https://github.com/PHPMailer/PHPMailer/issues/1299\"  ...  i posted this issue on so: https://stackoverflow.com/questions/47916568/unintended-indents-with-plain-text-email in trying to figure out what the problem was, i theorized that it was related to the character set always defaulting to us-ascii as posted by the so user: https://stackoverflow.com/questions/43022838/phpmailer-altbody-plain-text-version-of-message-has-always-us-ascii-charset basically, i think that this kind of thing... //can we do a 7-bit downgrade? if '8bit' == $bodyencoding and...\n",
              "5238497      \"https://github.com/attic-labs/noms/issues/3322\"  ...                                     i saw that issue 2327 is now closed, which is an item on the roadmap https://github.com/attic-labs/noms roadmap . while the issue is closed, it was not checked off on the roadmap. the static nature of the roadmap makes this an easy oversight. i think github's milestones exists for the exact purpose for which you've created the roadmap and it will never report false status ; my apologies for the pedantic and low priority issues that i've been raising this week!\n",
              "5230334    \"https://github.com/aantron/bisect_ppx/issues/143\"  ...                                                                                                                                                                    this is to eliminate the depopt https://github.com/aantron/bisect_ppx/blob/07a994f6bbe8724ad62e77b75f2c326810da33ea/bisect_ppx.opam l33 :scream: we temporarily got as part of 117, and is a follow-on to that pr. we should release once with the depopt, but with the new package, and warn people using the ocamlbuild plugin to switch to it.\n",
              "\n",
              "[3 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmLJciac8PPF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "37ccb3ad-3abc-4849-dd23-b83af22edb54"
      },
      "source": [
        "train_body_raw = traindf.body.tolist()\n",
        "train_title_raw = traindf.issue_title.tolist()\n",
        "#preview output of first element\n",
        "train_body_raw[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"i posted this issue on so: https://stackoverflow.com/questions/47916568/unintended-indents-with-plain-text-email in trying to figure out what the problem was, i theorized that it was related to the character set always defaulting to us-ascii as posted by the so user: https://stackoverflow.com/questions/43022838/phpmailer-altbody-plain-text-version-of-message-has-always-us-ascii-charset basically, i think that this kind of thing... //can we do a 7-bit downgrade? if '8bit' == $bodyencoding and !$this->has8bitchars $this->body { $bodyencoding = '7bit'; all iso 8859, windows codepage and utf-8 charsets are ascii compatible up to 7-bit $bodycharset = 'us-ascii'; } overrides the user's intended charset. if you do $mail->charset = 'utf-8'; , then depending on the type of content in the email, the choice of utf-8 can be overridden and us-ascii will be used. is this the intended behavior? fyi: there are two instances of this kind of logic in phpmailer.php and also two in class.phpmailer.php\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9dwQqxI99MM"
      },
      "source": [
        "!pip install -q ktext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voOOuY3o8ilV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "06569a32-f281-4608-9319-e871e6a9f9cc"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "from ktext.preprocess import processor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBbFBe6U-hdo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "00759843-b71c-4807-c151-fdd59f44b6ad"
      },
      "source": [
        "%%time\n",
        "# Clean, tokenize, and apply padding / truncating such that each document length = 70\n",
        "#  also, retain only the top 8,000 words in the vocabulary and set the remaining words\n",
        "#  to 1 which will become common index for rare words \n",
        "body_pp = processor(keep_n=8000, padding_maxlen=70)\n",
        "train_body_vecs = body_pp.fit_transform(train_body_raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:(1/2) done. 0 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 0 sec\n",
            "WARNING:root:Finished parsing 180 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 0 sec\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 44.7 ms, sys: 170 ms, total: 214 ms\n",
            "Wall time: 547 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewqvp1kC8wFH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "f9ef4699-8a50-4d79-92ac-506e1a897d80"
      },
      "source": [
        "print('\\noriginal string:\\n', train_body_raw[0], '\\n')\n",
        "print('after pre-processing:\\n', train_body_vecs[0], '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "original string:\n",
            " i posted this issue on so: https://stackoverflow.com/questions/47916568/unintended-indents-with-plain-text-email in trying to figure out what the problem was, i theorized that it was related to the character set always defaulting to us-ascii as posted by the so user: https://stackoverflow.com/questions/43022838/phpmailer-altbody-plain-text-version-of-message-has-always-us-ascii-charset basically, i think that this kind of thing... //can we do a 7-bit downgrade? if '8bit' == $bodyencoding and !$this->has8bitchars $this->body { $bodyencoding = '7bit'; all iso 8859, windows codepage and utf-8 charsets are ascii compatible up to 7-bit $bodycharset = 'us-ascii'; } overrides the user's intended charset. if you do $mail->charset = 'utf-8'; , then depending on the type of content in the email, the choice of utf-8 can be overridden and us-ascii will be used. is this the intended behavior? fyi: there are two instances of this kind of logic in phpmailer.php and also two in class.phpmailer.php \n",
            "\n",
            "after pre-processing:\n",
            " [   6  784   13   36   14   37   12    7  291    4  785  118   63    2\n",
            "  292   64    6 1273   15   10   64  422    4    2 1274   83  551 1275\n",
            "    4  202  423   20  784   40    2   37   56   12  786    6  165   15\n",
            "   13  356    9  357   21   26   49    5    3  203 1276   28 1277  787\n",
            "    8  788 1278  788  789  787 1279   45 1280    3  149 1281    8  552] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_1oV7r9AuP-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "23792149-d0d0-4cc2-8d3a-b3a4c2a29959"
      },
      "source": [
        "# Instantiate a text processor for the titles, with some different parameters\n",
        "#  append_indicators = True appends the tokens '_start_' and '_end_' to each\n",
        "#                      document\n",
        "#  padding = 'post' means that zero padding is appended to the end of the \n",
        "#             of the document (as opposed to the default which is 'pre')\n",
        "title_pp = processor(append_indicators=True, keep_n=4500, \n",
        "                     padding_maxlen=12, padding ='post')\n",
        "\n",
        "# process the title data\n",
        "train_title_vecs = title_pp.fit_transform(train_title_raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:(1/2) done. 0 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 0 sec\n",
            "WARNING:root:Finished parsing 180 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 0 sec\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-bUZzD_AyN_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "991bc14b-b230-4fc8-a3ac-63dfa70f190d"
      },
      "source": [
        "print('\\noriginal string:\\n', train_title_raw[0])\n",
        "print('after pre-processing:\\n', train_title_vecs[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "original string:\n",
            " possible issue with doing a 7-bit downgrade?\n",
            "after pre-processing:\n",
            " [  2  51  14  12 177   8   5 178 179   3   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NHH5ySQA27a"
      },
      "source": [
        "import dill as dpickle\n",
        "import numpy as np\n",
        "\n",
        "# Save the preprocessor\n",
        "with open('body_pp.dpkl', 'wb') as f:\n",
        "    dpickle.dump(body_pp, f)\n",
        "\n",
        "with open('title_pp.dpkl', 'wb') as f:\n",
        "    dpickle.dump(title_pp, f)\n",
        "\n",
        "# Save the processed data\n",
        "np.save('train_title_vecs.npy', train_title_vecs)\n",
        "np.save('train_body_vecs.npy', train_body_vecs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luUfvoG2GkX9"
      },
      "source": [
        "!pip install -q annoy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNyaCTpnA4_v"
      },
      "source": [
        "!cp \"/content/drive/My Drive/natural-language-processing/seq2seq_utils.py\" .\n",
        "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmvwDw_sC7jI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a2045dfd-eae1-4393-a8bd-1486af233c58"
      },
      "source": [
        "encoder_input_data, doc_length = load_encoder_inputs('train_body_vecs.npy')\n",
        "decoder_input_data, decoder_target_data = load_decoder_inputs('train_title_vecs.npy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of encoder input: (180, 70)\n",
            "Shape of decoder input: (180, 11)\n",
            "Shape of decoder target: (180, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCEhGA44GLrr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c2bc3f49-02c6-4eca-9cc2-c6fea1c62a19"
      },
      "source": [
        "num_encoder_tokens, body_pp = load_text_processor('body_pp.dpkl')\n",
        "num_decoder_tokens, title_pp = load_text_processor('title_pp.dpkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of vocabulary for body_pp.dpkl: 2,821\n",
            "Size of vocabulary for title_pp.dpkl: 734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snRlFsR3HFWs"
      },
      "source": [
        "%matplotlib inline\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
        "from keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2gpn01DHF3E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "64fa25ec-488d-49fc-bd56-b25023045cd8"
      },
      "source": [
        "#arbitrarly set latent dimension for embedding and hidden units\n",
        "latent_dim = 300\n",
        "\n",
        "##### Define Model Architecture ######\n",
        "\n",
        "########################\n",
        "#### Encoder Model ####\n",
        "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
        "\n",
        "# Word embeding for encoder (ex: Issue Body)\n",
        "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
        "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
        "\n",
        "# Intermediate GRU layer (optional)\n",
        "#x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
        "#x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
        "\n",
        "# We do not need the `encoder_output` just the hidden state.\n",
        "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
        "\n",
        "# Encapsulate the encoder as a separate entity so we can just \n",
        "#  encode without decoding if we want to.\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
        "\n",
        "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
        "\n",
        "########################\n",
        "#### Decoder Model ####\n",
        "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
        "\n",
        "# Word Embedding For Decoder (ex: Issue Titles)\n",
        "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
        "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
        "\n",
        "# Set up the decoder, using `decoder_state_input` as initial state.\n",
        "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
        "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
        "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
        "\n",
        "# Dense layer for prediction\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
        "decoder_outputs = decoder_dense(x)\n",
        "\n",
        "########################\n",
        "#### Seq2Seq Model ####\n",
        "\n",
        "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
        "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "\n",
        "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0chbs8dHImR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "outputId": "e4c9ebbb-a396-45b4-9d71-ddfa4e488fd9"
      },
      "source": [
        "from seq2seq_utils import viz_model_architecture\n",
        "seq2seq_Model.summary()\n",
        "viz_model_architecture(seq2seq_Model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Word-Embedding (Embeddi (None, None, 300)    220200      Decoder-Input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Input (InputLayer)      (None, 70)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Batchnorm-1 (BatchNorma (None, None, 300)    1200        Decoder-Word-Embedding[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Model (Model)           (None, 300)          1388400     Encoder-Input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-GRU (GRU)               [(None, None, 300),  540900      Decoder-Batchnorm-1[0][0]        \n",
            "                                                                 Encoder-Model[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoder-Batchnorm-2 (BatchNorma (None, None, 300)    1200        Decoder-GRU[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Final-Output-Dense (Dense)      (None, None, 734)    220934      Decoder-Batchnorm-2[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 2,372,834\n",
            "Trainable params: 2,371,034\n",
            "Non-trainable params: 1,800\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"410pt\" viewBox=\"0.00 0.00 455.00 410.00\" width=\"455pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 406)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-406 451,-406 451,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140432047751240 -->\n<g class=\"node\" id=\"node1\">\n<title>140432047751240</title>\n<polygon fill=\"none\" points=\"48,-365.5 48,-401.5 219,-401.5 219,-365.5 48,-365.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-379.8\">Decoder-Input: InputLayer</text>\n</g>\n<!-- 140432047754880 -->\n<g class=\"node\" id=\"node2\">\n<title>140432047754880</title>\n<polygon fill=\"none\" points=\"9.5,-292.5 9.5,-328.5 257.5,-328.5 257.5,-292.5 9.5,-292.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-306.8\">Decoder-Word-Embedding: Embedding</text>\n</g>\n<!-- 140432047751240&#45;&gt;140432047754880 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140432047751240-&gt;140432047754880</title>\n<path d=\"M133.5,-365.4551C133.5,-357.3828 133.5,-347.6764 133.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"137.0001,-338.5903 133.5,-328.5904 130.0001,-338.5904 137.0001,-338.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140432047822720 -->\n<g class=\"node\" id=\"node4\">\n<title>140432047822720</title>\n<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 267,-255.5 267,-219.5 0,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-233.8\">Decoder-Batchnorm-1: BatchNormalization</text>\n</g>\n<!-- 140432047754880&#45;&gt;140432047822720 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140432047754880-&gt;140432047822720</title>\n<path d=\"M133.5,-292.4551C133.5,-284.3828 133.5,-274.6764 133.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"137.0001,-265.5903 133.5,-255.5904 130.0001,-265.5904 137.0001,-265.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140432027374704 -->\n<g class=\"node\" id=\"node3\">\n<title>140432027374704</title>\n<polygon fill=\"none\" points=\"276,-292.5 276,-328.5 447,-328.5 447,-292.5 276,-292.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"361.5\" y=\"-306.8\">Encoder-Input: InputLayer</text>\n</g>\n<!-- 140435602477464 -->\n<g class=\"node\" id=\"node5\">\n<title>140435602477464</title>\n<polygon fill=\"none\" points=\"285.5,-219.5 285.5,-255.5 437.5,-255.5 437.5,-219.5 285.5,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"361.5\" y=\"-233.8\">Encoder-Model: Model</text>\n</g>\n<!-- 140432027374704&#45;&gt;140435602477464 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140432027374704-&gt;140435602477464</title>\n<path d=\"M361.5,-292.4551C361.5,-284.3828 361.5,-274.6764 361.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"365.0001,-265.5903 361.5,-255.5904 358.0001,-265.5904 365.0001,-265.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140432655328200 -->\n<g class=\"node\" id=\"node6\">\n<title>140432655328200</title>\n<polygon fill=\"none\" points=\"178.5,-146.5 178.5,-182.5 316.5,-182.5 316.5,-146.5 178.5,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247.5\" y=\"-160.8\">Decoder-GRU: GRU</text>\n</g>\n<!-- 140432047822720&#45;&gt;140432655328200 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140432047822720-&gt;140432655328200</title>\n<path d=\"M161.6798,-219.4551C176.478,-209.979 194.7933,-198.2508 210.6986,-188.0658\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"212.7154,-190.9305 219.2493,-182.5904 208.9405,-185.0356 212.7154,-190.9305\" stroke=\"#000000\"/>\n</g>\n<!-- 140435602477464&#45;&gt;140432655328200 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140435602477464-&gt;140432655328200</title>\n<path d=\"M333.3202,-219.4551C318.522,-209.979 300.2067,-198.2508 284.3014,-188.0658\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"286.0595,-185.0356 275.7507,-182.5904 282.2846,-190.9305 286.0595,-185.0356\" stroke=\"#000000\"/>\n</g>\n<!-- 140432732011600 -->\n<g class=\"node\" id=\"node7\">\n<title>140432732011600</title>\n<polygon fill=\"none\" points=\"114,-73.5 114,-109.5 381,-109.5 381,-73.5 114,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247.5\" y=\"-87.8\">Decoder-Batchnorm-2: BatchNormalization</text>\n</g>\n<!-- 140432655328200&#45;&gt;140432732011600 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140432655328200-&gt;140432732011600</title>\n<path d=\"M247.5,-146.4551C247.5,-138.3828 247.5,-128.6764 247.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"251.0001,-119.5903 247.5,-109.5904 244.0001,-119.5904 251.0001,-119.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140432731786488 -->\n<g class=\"node\" id=\"node8\">\n<title>140432731786488</title>\n<polygon fill=\"none\" points=\"161,-.5 161,-36.5 334,-36.5 334,-.5 161,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247.5\" y=\"-14.8\">Final-Output-Dense: Dense</text>\n</g>\n<!-- 140432732011600&#45;&gt;140432731786488 -->\n<g class=\"edge\" id=\"edge7\">\n<title>140432732011600-&gt;140432731786488</title>\n<path d=\"M247.5,-73.4551C247.5,-65.3828 247.5,-55.6764 247.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"251.0001,-46.5903 247.5,-36.5904 244.0001,-46.5904 251.0001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMr-iI-R2ph3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "93943ca4-5eb1-4a8b-f691-7db4c5291784"
      },
      "source": [
        "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "\n",
        "script_name_base = 'tutorial_seq2seq'\n",
        "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
        "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
        "                                   save_best_only=True)\n",
        "\n",
        "batch_size = 1200\n",
        "epochs = 7\n",
        "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 158 samples, validate on 22 samples\n",
            "Epoch 1/7\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r158/158 [==============================] - 4s 27ms/step - loss: 6.8535 - val_loss: 5.3847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/7\n",
            "158/158 [==============================] - 1s 9ms/step - loss: 4.7829 - val_loss: 4.9586\n",
            "Epoch 3/7\n",
            "158/158 [==============================] - 1s 9ms/step - loss: 4.0784 - val_loss: 4.7196\n",
            "Epoch 4/7\n",
            "158/158 [==============================] - 1s 9ms/step - loss: 3.5762 - val_loss: 4.5855\n",
            "Epoch 5/7\n",
            "158/158 [==============================] - 1s 9ms/step - loss: 3.1822 - val_loss: 4.5152\n",
            "Epoch 6/7\n",
            "158/158 [==============================] - 1s 9ms/step - loss: 2.8666 - val_loss: 4.4667\n",
            "Epoch 7/7\n",
            "158/158 [==============================] - 1s 9ms/step - loss: 2.5371 - val_loss: 4.4391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_WHyyHQ210a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7ca648df-bb03-43da-deb3-2d8791da2f76"
      },
      "source": [
        "#save model\n",
        "seq2seq_Model.save('seq2seq_model_tutorial.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xub6i_vq28xr"
      },
      "source": [
        "from seq2seq_utils import Seq2Seq_Inference\n",
        "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
        "                                 decoder_preprocessor=title_pp,\n",
        "                                 seq2seq_model=seq2seq_Model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4B-pbm33ThG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "709ff0bd-3286-4ff2-ddaa-7f766cbedcf7"
      },
      "source": [
        "# this method displays the predictions on random rows of the holdout set\n",
        "seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 6 =================\n",
            "\n",
            "\"https://github.com/Baystation12/Baystation12/issues/16021\"\n",
            "Issue Body:\n",
            " not intentional behaviour, but is actually pretty cool thing - you can move down on open space with lattice. i think it should stay, but visible message / delay should be added for consistency with ladders imo. \n",
            "\n",
            "Original Title:\n",
            " climbing down lattices is instant\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " replace roadmap readme with event\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 6 =================\n",
            "\n",
            "\"https://github.com/Baystation12/Baystation12/issues/16021\"\n",
            "Issue Body:\n",
            " not intentional behaviour, but is actually pretty cool thing - you can move down on open space with lattice. i think it should stay, but visible message / delay should be added for consistency with ladders imo. \n",
            "\n",
            "Original Title:\n",
            " climbing down lattices is instant\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " replace roadmap readme with event\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 3 =================\n",
            "\n",
            "\"https://github.com/Vanhal/ProgressiveAutomation/issues/324\"\n",
            "Issue Body:\n",
            " obviously this is two mods working together and you may not have added compatability but when useing the crafting to craft the metals that need a fluid in thermal foundation signalum, lumium, enderium you can interchange any fluid and it will still make them. for instance you can craft signalum with tree oil. i understand this may not be a mod you added compatability for so its not a massive issuse just wanted to make you aware of it incase you did decide to add compatability \n",
            "\n",
            "Original Title:\n",
            " thermal foundation fluids, issuse with crafter\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 18 =================\n",
            "\n",
            "\"https://github.com/service-portal/CCW3955/issues/1\"\n",
            "Issue Body:\n",
            " anxious to try this out; however, i'm thinking it might only run in jakarta. so i've got two ideas: can you enhance the code so it works on istanbul ? or perhaps give me access to jakarta? i'll assume the former : repo loads fine into istanbul studio. when i load the /d1 page, i get the following error: function addtrend is not allowed in scope x_snc_dashboard ! image https://cloud.githubusercontent.com/assets/9342308/25872547/3223ec1c-34d9-11e7-9e59-6ea46fe39474.png \n",
            "\n",
            "Original Title:\n",
            " doesn't render in istanbul\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " replace roadmap readme with event\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 8 =================\n",
            "\n",
            "\"https://github.com/githubschool/open-enrollment-classes-introduction-to-github/issues/11391\"\n",
            "Issue Body:\n",
            " hello! :wave: please add me as a collaborator to this repo. \n",
            "\n",
            "Original Title:\n",
            " i think i found a bug with my first submitted issue\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " add standalone electron or light\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 10 =================\n",
            "\n",
            "\"https://github.com/vitalidze/traccar-web/issues/1019\"\n",
            "Issue Body:\n",
            " hello everybody have a good new year. i have this issue: all notificacions are sent with this subject: traccar-web notification. i change the template subject and body of nofitications but it still send with the subject wrong. \n",
            "\n",
            "Original Title:\n",
            " subject in all email is: traccar-web notification\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 11 =================\n",
            "\n",
            "\"https://github.com/benwbrum/fromthepage/issues/677\"\n",
            "Issue Body:\n",
            " move owner_email_blacklist = from send_owner_emails.rake to the 00fromthepage initializer. question: that would require a restart. is there a way to do this without a restart? i guess the db... \n",
            "\n",
            "Original Title:\n",
            " move owner_email_blacklist to initializer\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 14 =================\n",
            "\n",
            "\"https://github.com/benweet/stackedit/issues/1071\"\n",
            "Issue Body:\n",
            " last commit was about a year ago -- perhaps there is a fork that's actively maintained? \n",
            "\n",
            "Original Title:\n",
            " is stackedit is being maintained?\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " replace roadmap readme with event\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 17 =================\n",
            "\n",
            "\"https://github.com/facebook/react-native/issues/14472\"\n",
            "Issue Body:\n",
            " package.json { name : haha , version : 0.0.1 , private : true, scripts : { start : node node_modules/react-native/local-cli/cli.js start , test : jest }, dependencies : { react : 16.0.0-alpha.6 , react-native : 0.44.3 }, devdependencies : { babel-jest : 20.0.3 , babel-preset-react-native : 1.9.2 , jest : 20.0.4 , react-test-renderer : 16.0.0-alpha.6 }, jest : { preset : react-native } } it's root: working ! image https://user-images.githubusercontent.com/1804755/27042723-6a3509c2-4fca-11e7-8663-f4b5b6f0cb95.png it's nested: not working ! image https://user-images.githubusercontent.com/1804755/27042783-959c5f16-4fca-11e7-99fd-7cb137be7013.png why? \n",
            "\n",
            "Original Title:\n",
            " why webview only work when it's the root element? is this expected behavior?\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " disk depricated google compute engine\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 5 =================\n",
            "\n",
            "\"https://github.com/firemodels/smv/issues/207\"\n",
            "Issue Body:\n",
            " the sphere used to distinguish hvac vents from regular vents is not always drawn with an appropriate size to large or too small depending on the size of the compartments being modled \n",
            "\n",
            "Original Title:\n",
            " problem with zone hvac vent display\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 10 =================\n",
            "\n",
            "\"https://github.com/vitalidze/traccar-web/issues/1019\"\n",
            "Issue Body:\n",
            " hello everybody have a good new year. i have this issue: all notificacions are sent with this subject: traccar-web notification. i change the template subject and body of nofitications but it still send with the subject wrong. \n",
            "\n",
            "Original Title:\n",
            " subject in all email is: traccar-web notification\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 5 =================\n",
            "\n",
            "\"https://github.com/firemodels/smv/issues/207\"\n",
            "Issue Body:\n",
            " the sphere used to distinguish hvac vents from regular vents is not always drawn with an appropriate size to large or too small depending on the size of the compartments being modled \n",
            "\n",
            "Original Title:\n",
            " problem with zone hvac vent display\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 1 =================\n",
            "\n",
            "\"https://github.com/PaddlePaddle/PaddlePaddle.org/issues/43\"\n",
            "Issue Body:\n",
            " i am trying to run the portal locally by following this document https://github.com/paddlepaddle/paddlepaddle.org/blob/develop/install.md run-docker-image . however, i stopped at where a secret_key=<secret_key> is required. how can i have this secret_key ? also, docker run requires -v <external_template_dir>:/var/content how can i have that external_template_dir ? thanks! - yi \n",
            "\n",
            "Original Title:\n",
            " how to run in docker container\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 18 =================\n",
            "\n",
            "\"https://github.com/service-portal/CCW3955/issues/1\"\n",
            "Issue Body:\n",
            " anxious to try this out; however, i'm thinking it might only run in jakarta. so i've got two ideas: can you enhance the code so it works on istanbul ? or perhaps give me access to jakarta? i'll assume the former : repo loads fine into istanbul studio. when i load the /d1 page, i get the following error: function addtrend is not allowed in scope x_snc_dashboard ! image https://cloud.githubusercontent.com/assets/9342308/25872547/3223ec1c-34d9-11e7-9e59-6ea46fe39474.png \n",
            "\n",
            "Original Title:\n",
            " doesn't render in istanbul\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " replace roadmap readme with event\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 2 =================\n",
            "\n",
            "\"https://github.com/datosgobar/portal-base/issues/21\"\n",
            "Issue Body:\n",
            " configurar los logs de las aplicaciones apache, nginx, postgres para que usen logrotate \n",
            "\n",
            "Original Title:\n",
            " evitar crecimiento infinito de los logs en los contenedores\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 1 =================\n",
            "\n",
            "\"https://github.com/PaddlePaddle/PaddlePaddle.org/issues/43\"\n",
            "Issue Body:\n",
            " i am trying to run the portal locally by following this document https://github.com/paddlepaddle/paddlepaddle.org/blob/develop/install.md run-docker-image . however, i stopped at where a secret_key=<secret_key> is required. how can i have this secret_key ? also, docker run requires -v <external_template_dir>:/var/content how can i have that external_template_dir ? thanks! - yi \n",
            "\n",
            "Original Title:\n",
            " how to run in docker container\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 5 =================\n",
            "\n",
            "\"https://github.com/firemodels/smv/issues/207\"\n",
            "Issue Body:\n",
            " the sphere used to distinguish hvac vents from regular vents is not always drawn with an appropriate size to large or too small depending on the size of the compartments being modled \n",
            "\n",
            "Original Title:\n",
            " problem with zone hvac vent display\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 14 =================\n",
            "\n",
            "\"https://github.com/benweet/stackedit/issues/1071\"\n",
            "Issue Body:\n",
            " last commit was about a year ago -- perhaps there is a fork that's actively maintained? \n",
            "\n",
            "Original Title:\n",
            " is stackedit is being maintained?\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " replace roadmap readme with event\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 3 =================\n",
            "\n",
            "\"https://github.com/Vanhal/ProgressiveAutomation/issues/324\"\n",
            "Issue Body:\n",
            " obviously this is two mods working together and you may not have added compatability but when useing the crafting to craft the metals that need a fluid in thermal foundation signalum, lumium, enderium you can interchange any fluid and it will still make them. for instance you can craft signalum with tree oil. i understand this may not be a mod you added compatability for so its not a massive issuse just wanted to make you aware of it incase you did decide to add compatability \n",
            "\n",
            "Original Title:\n",
            " thermal foundation fluids, issuse with crafter\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 13 =================\n",
            "\n",
            "\"https://github.com/theforeman/foreman_monitoring/issues/17\"\n",
            "Issue Body:\n",
            " after icinga noticed a problem and me acknowledging it the problem remain ! selection_002 https://user-images.githubusercontent.com/2285225/27593437-7d29df7e-5b57-11e7-9271-d09c7e1e7c6b.png although they have long gone from the icinga notifications. the unackknowledged problems do disappear. \n",
            "\n",
            "Original Title:\n",
            " acknowledged problem stick\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 16 =================\n",
            "\n",
            "\"https://github.com/koorellasuresh/UKRegionTest/issues/65947\"\n",
            "Issue Body:\n",
            " first from flow in uk south \n",
            "\n",
            "Original Title:\n",
            " first from flow in uk south\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " first from from breusch pagan\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 11 =================\n",
            "\n",
            "\"https://github.com/benwbrum/fromthepage/issues/677\"\n",
            "Issue Body:\n",
            " move owner_email_blacklist = from send_owner_emails.rake to the 00fromthepage initializer. question: that would require a restart. is there a way to do this without a restart? i guess the db... \n",
            "\n",
            "Original Title:\n",
            " move owner_email_blacklist to initializer\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 19 =================\n",
            "\n",
            "\"https://github.com/mschmo/vscode-s3-interface/issues/6\"\n",
            "Issue Body:\n",
            " update the logic here: https://github.com/mschmo/vscode-s3-interface/blob/master/src/bucket-contents.ts l27 \n",
            "\n",
            "Original Title:\n",
            " needs to support nested folders\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " please change loading\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 7 =================\n",
            "\n",
            "\"https://github.com/gparmer/composite/issues/235\"\n",
            "Issue Body:\n",
            " we likely want a page that holds at the very least: - a preemption count this can be used for per-core implementations. this enables transaction rollback and efficient per-core data-structures. see some of the work at fb in linux on this. \n",
            "\n",
            "Original Title:\n",
            " add preemption count shared page\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " is it s april number number number number number number number number\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 19 =================\n",
            "\n",
            "\"https://github.com/mschmo/vscode-s3-interface/issues/6\"\n",
            "Issue Body:\n",
            " update the logic here: https://github.com/mschmo/vscode-s3-interface/blob/master/src/bucket-contents.ts l27 \n",
            "\n",
            "Original Title:\n",
            " needs to support nested folders\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " please change loading\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 6 =================\n",
            "\n",
            "\"https://github.com/Baystation12/Baystation12/issues/16021\"\n",
            "Issue Body:\n",
            " not intentional behaviour, but is actually pretty cool thing - you can move down on open space with lattice. i think it should stay, but visible message / delay should be added for consistency with ladders imo. \n",
            "\n",
            "Original Title:\n",
            " climbing down lattices is instant\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " replace roadmap readme with event\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 3 =================\n",
            "\n",
            "\"https://github.com/Vanhal/ProgressiveAutomation/issues/324\"\n",
            "Issue Body:\n",
            " obviously this is two mods working together and you may not have added compatability but when useing the crafting to craft the metals that need a fluid in thermal foundation signalum, lumium, enderium you can interchange any fluid and it will still make them. for instance you can craft signalum with tree oil. i understand this may not be a mod you added compatability for so its not a massive issuse just wanted to make you aware of it incase you did decide to add compatability \n",
            "\n",
            "Original Title:\n",
            " thermal foundation fluids, issuse with crafter\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 18 =================\n",
            "\n",
            "\"https://github.com/service-portal/CCW3955/issues/1\"\n",
            "Issue Body:\n",
            " anxious to try this out; however, i'm thinking it might only run in jakarta. so i've got two ideas: can you enhance the code so it works on istanbul ? or perhaps give me access to jakarta? i'll assume the former : repo loads fine into istanbul studio. when i load the /d1 page, i get the following error: function addtrend is not allowed in scope x_snc_dashboard ! image https://cloud.githubusercontent.com/assets/9342308/25872547/3223ec1c-34d9-11e7-9e59-6ea46fe39474.png \n",
            "\n",
            "Original Title:\n",
            " doesn't render in istanbul\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " replace roadmap readme with event\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 1 =================\n",
            "\n",
            "\"https://github.com/PaddlePaddle/PaddlePaddle.org/issues/43\"\n",
            "Issue Body:\n",
            " i am trying to run the portal locally by following this document https://github.com/paddlepaddle/paddlepaddle.org/blob/develop/install.md run-docker-image . however, i stopped at where a secret_key=<secret_key> is required. how can i have this secret_key ? also, docker run requires -v <external_template_dir>:/var/content how can i have that external_template_dir ? thanks! - yi \n",
            "\n",
            "Original Title:\n",
            " how to run in docker container\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 14 =================\n",
            "\n",
            "\"https://github.com/benweet/stackedit/issues/1071\"\n",
            "Issue Body:\n",
            " last commit was about a year ago -- perhaps there is a fork that's actively maintained? \n",
            "\n",
            "Original Title:\n",
            " is stackedit is being maintained?\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " replace roadmap readme with event\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 13 =================\n",
            "\n",
            "\"https://github.com/theforeman/foreman_monitoring/issues/17\"\n",
            "Issue Body:\n",
            " after icinga noticed a problem and me acknowledging it the problem remain ! selection_002 https://user-images.githubusercontent.com/2285225/27593437-7d29df7e-5b57-11e7-9271-d09c7e1e7c6b.png although they have long gone from the icinga notifications. the unackknowledged problems do disappear. \n",
            "\n",
            "Original Title:\n",
            " acknowledged problem stick\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 13 =================\n",
            "\n",
            "\"https://github.com/theforeman/foreman_monitoring/issues/17\"\n",
            "Issue Body:\n",
            " after icinga noticed a problem and me acknowledging it the problem remain ! selection_002 https://user-images.githubusercontent.com/2285225/27593437-7d29df7e-5b57-11e7-9271-d09c7e1e7c6b.png although they have long gone from the icinga notifications. the unackknowledged problems do disappear. \n",
            "\n",
            "Original Title:\n",
            " acknowledged problem stick\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 19 =================\n",
            "\n",
            "\"https://github.com/mschmo/vscode-s3-interface/issues/6\"\n",
            "Issue Body:\n",
            " update the logic here: https://github.com/mschmo/vscode-s3-interface/blob/master/src/bucket-contents.ts l27 \n",
            "\n",
            "Original Title:\n",
            " needs to support nested folders\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " please change loading\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 18 =================\n",
            "\n",
            "\"https://github.com/service-portal/CCW3955/issues/1\"\n",
            "Issue Body:\n",
            " anxious to try this out; however, i'm thinking it might only run in jakarta. so i've got two ideas: can you enhance the code so it works on istanbul ? or perhaps give me access to jakarta? i'll assume the former : repo loads fine into istanbul studio. when i load the /d1 page, i get the following error: function addtrend is not allowed in scope x_snc_dashboard ! image https://cloud.githubusercontent.com/assets/9342308/25872547/3223ec1c-34d9-11e7-9e59-6ea46fe39474.png \n",
            "\n",
            "Original Title:\n",
            " doesn't render in istanbul\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " replace roadmap readme with event\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 7 =================\n",
            "\n",
            "\"https://github.com/gparmer/composite/issues/235\"\n",
            "Issue Body:\n",
            " we likely want a page that holds at the very least: - a preemption count this can be used for per-core implementations. this enables transaction rollback and efficient per-core data-structures. see some of the work at fb in linux on this. \n",
            "\n",
            "Original Title:\n",
            " add preemption count shared page\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " is it s april number number number number number number number number\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 7 =================\n",
            "\n",
            "\"https://github.com/gparmer/composite/issues/235\"\n",
            "Issue Body:\n",
            " we likely want a page that holds at the very least: - a preemption count this can be used for per-core implementations. this enables transaction rollback and efficient per-core data-structures. see some of the work at fb in linux on this. \n",
            "\n",
            "Original Title:\n",
            " add preemption count shared page\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " is it s april number number number number number number number number\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 14 =================\n",
            "\n",
            "\"https://github.com/benweet/stackedit/issues/1071\"\n",
            "Issue Body:\n",
            " last commit was about a year ago -- perhaps there is a fork that's actively maintained? \n",
            "\n",
            "Original Title:\n",
            " is stackedit is being maintained?\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " replace roadmap readme with event\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 14 =================\n",
            "\n",
            "\"https://github.com/benweet/stackedit/issues/1071\"\n",
            "Issue Body:\n",
            " last commit was about a year ago -- perhaps there is a fork that's actively maintained? \n",
            "\n",
            "Original Title:\n",
            " is stackedit is being maintained?\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " replace roadmap readme with event\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 19 =================\n",
            "\n",
            "\"https://github.com/mschmo/vscode-s3-interface/issues/6\"\n",
            "Issue Body:\n",
            " update the logic here: https://github.com/mschmo/vscode-s3-interface/blob/master/src/bucket-contents.ts l27 \n",
            "\n",
            "Original Title:\n",
            " needs to support nested folders\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " please change loading\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 16 =================\n",
            "\n",
            "\"https://github.com/koorellasuresh/UKRegionTest/issues/65947\"\n",
            "Issue Body:\n",
            " first from flow in uk south \n",
            "\n",
            "Original Title:\n",
            " first from flow in uk south\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " first from from breusch pagan\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 19 =================\n",
            "\n",
            "\"https://github.com/mschmo/vscode-s3-interface/issues/6\"\n",
            "Issue Body:\n",
            " update the logic here: https://github.com/mschmo/vscode-s3-interface/blob/master/src/bucket-contents.ts l27 \n",
            "\n",
            "Original Title:\n",
            " needs to support nested folders\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " please change loading\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 16 =================\n",
            "\n",
            "\"https://github.com/koorellasuresh/UKRegionTest/issues/65947\"\n",
            "Issue Body:\n",
            " first from flow in uk south \n",
            "\n",
            "Original Title:\n",
            " first from flow in uk south\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " first from from breusch pagan\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 4 =================\n",
            "\n",
            "\"https://github.com/mcuadros/terraform-provider-helm/issues/13\"\n",
            "Issue Body:\n",
            " helm 2.6.0 contains a lot of bug fixes that bit me during 2.5.x, one of them is related to cert_file and key_file based client authentication, where the ca_file is not optional: https://github.com/kubernetes/helm/commit/5f96fb816c42b85813a7c158c9d072836f91c395 \n",
            "\n",
            "Original Title:\n",
            " can we vendor helm 2.6.0?\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " cannot check shared drives\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 8 =================\n",
            "\n",
            "\"https://github.com/githubschool/open-enrollment-classes-introduction-to-github/issues/11391\"\n",
            "Issue Body:\n",
            " hello! :wave: please add me as a collaborator to this repo. \n",
            "\n",
            "Original Title:\n",
            " i think i found a bug with my first submitted issue\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " add standalone electron or light\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 12 =================\n",
            "\n",
            "\"https://github.com/DillonN/NiceHashMiner/issues/24\"\n",
            "Issue Body:\n",
            " to the next week pls : i tryd to just copy folders from the new versions into the dillonn mod, but it doesnt work!? pls add the new miner updates for your mod.. thx a lot : \n",
            "\n",
            "Original Title:\n",
            " pls add new claymore, ethminer nvidia optimized & excavator cuda optimized ..\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " job ui console is to problem number number number number number number\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 9 =================\n",
            "\n",
            "\"https://github.com/TheMegaTB/BJS/issues/48\"\n",
            "Issue Body:\n",
            " add/update sport type description to data/ /sports.json. for most sport types the official document https://drive.google.com/drive/folders/0b0zl5evdl1cqb1ver1d0ngh3dtq includes a description. html tags are allowed. images are desirable. \n",
            "\n",
            "Original Title:\n",
            " add/update sport type description\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 4 =================\n",
            "\n",
            "\"https://github.com/mcuadros/terraform-provider-helm/issues/13\"\n",
            "Issue Body:\n",
            " helm 2.6.0 contains a lot of bug fixes that bit me during 2.5.x, one of them is related to cert_file and key_file based client authentication, where the ca_file is not optional: https://github.com/kubernetes/helm/commit/5f96fb816c42b85813a7c158c9d072836f91c395 \n",
            "\n",
            "Original Title:\n",
            " can we vendor helm 2.6.0?\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " cannot check shared drives\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 12 =================\n",
            "\n",
            "\"https://github.com/DillonN/NiceHashMiner/issues/24\"\n",
            "Issue Body:\n",
            " to the next week pls : i tryd to just copy folders from the new versions into the dillonn mod, but it doesnt work!? pls add the new miner updates for your mod.. thx a lot : \n",
            "\n",
            "Original Title:\n",
            " pls add new claymore, ethminer nvidia optimized & excavator cuda optimized ..\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " job ui console is to problem number number number number number number\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 9 =================\n",
            "\n",
            "\"https://github.com/TheMegaTB/BJS/issues/48\"\n",
            "Issue Body:\n",
            " add/update sport type description to data/ /sports.json. for most sport types the official document https://drive.google.com/drive/folders/0b0zl5evdl1cqb1ver1d0ngh3dtq includes a description. html tags are allowed. images are desirable. \n",
            "\n",
            "Original Title:\n",
            " add/update sport type description\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 2 =================\n",
            "\n",
            "\"https://github.com/datosgobar/portal-base/issues/21\"\n",
            "Issue Body:\n",
            " configurar los logs de las aplicaciones apache, nginx, postgres para que usen logrotate \n",
            "\n",
            "Original Title:\n",
            " evitar crecimiento infinito de los logs en los contenedores\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK-QRfb33Zum"
      },
      "source": [
        "# Read All 5M data points\n",
        "all_data_df = pd.read_csv('/content/drive/My Drive/natural-language-processing/github_issues.csv').sample(n=200)\n",
        "# Extract the bodies from this dataframe\n",
        "all_data_bodies = all_data_df['body'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJZOBcB43tx-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0946a310-8c35-424a-d6a7-513c9a7f5b8a"
      },
      "source": [
        "# transform all of the data using the ktext processor\n",
        "all_data_vectorized = body_pp.transform_parallel(all_data_bodies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:...tokenizing data\n",
            "WARNING:root:...indexing data\n",
            "WARNING:root:...padding data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku06urez4GE4"
      },
      "source": [
        "# save transformed data\n",
        "with open('all_data_vectorized.dpkl', 'wb') as f:\n",
        "    dpickle.dump(all_data_vectorized, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJcTAjawFHjb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a6b0100c-297f-4f82-8667-175cb2c6ae1e"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "from seq2seq_utils import Seq2Seq_Inference\n",
        "seq2seq_inf_rec = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
        "                                    decoder_preprocessor=title_pp,\n",
        "                                    seq2seq_model=seq2seq_Model)\n",
        "recsys_annoyobj = seq2seq_inf_rec.prepare_recommender(all_data_vectorized, all_data_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/seq2seq_utils.py:380: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
            "  self.nn = AnnoyIndex(f)\n",
            "WARNING:root:Adding embeddings\n",
            "100%|██████████| 200/200 [00:00<00:00, 17218.35it/s]\n",
            "WARNING:root:Building trees for similarity lookup.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nkg-gabFGL8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "outputId": "af91a4cb-cde1-442f-812a-13b1623cf965"
      },
      "source": [
        "seq2seq_inf_rec.demo_model_predictions(n=1, issue_df=testdf, threshold=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 15 =================\n",
            "\n",
            "\"https://github.com/wandersonwhcr/balance/issues/209\"\n",
            "Issue Body:\n",
            " com a migração da máquina virtual do vagrant de wheezy para jessie , esquecemos de atualizar as dependências do apt-get , onde este deve capturar os pacotes do repositório específico da distribuição. este erro foi encontrado após verificar que o postgresql não está sendo inicializado junto com o sistema operacional. \n",
            "\n",
            "Original Title:\n",
            " atualizar dependências do jessie\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " replace roadmap readme with event\n",
            "\n",
            "**** Similar Issues (using encoder embedding) ****:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_url</th>\n",
              "      <th>issue_title</th>\n",
              "      <th>body</th>\n",
              "      <th>dist</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5016538</th>\n",
              "      <td>\"https://github.com/SvanteAlnas/Test/issues/29\"</td>\n",
              "      <td>omna-registration of lwm2m security</td>\n",
              "      <td>&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset= utf-8 /&gt;&lt;/head&gt;&lt;body&gt;&lt;table&gt;&lt;tr&gt;&lt;td&gt;new omna registration request&lt;/td&gt;&lt;td&gt;2017-12-27 13:14:38 utc&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;submitter's name&lt;/td&gt;&lt;td&gt;test&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;defined by&lt;/td&gt;&lt;td&gt;oma working group&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;type of registration&lt;/td&gt;&lt;td&gt;object&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;object name&lt;/td&gt;&lt;td&gt;lwm2m security&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;object description&lt;/td&gt;&lt;td&gt;test&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;link to object&lt;/td&gt;&lt;td&gt;&lt;a href= https://remote.alnas.xyz:8210/oeditor/modownloadzip?...</td>\n",
              "      <td>0.217566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465910</th>\n",
              "      <td>\"https://github.com/Kotlin/kotlinx.coroutines/issues/67\"</td>\n",
              "      <td>withtimeoutornull returns null even when it did not timeout itself</td>\n",
              "      <td>copied from message by @gregschlom at public slack: hey guys. i have an issue with nested withtimeouts. here’s some example code: val channel = channel&lt;int&gt; // this blocks indefinitely if the channel is empty suspend fun nexvalue : int { println waiting for next value return channel.receive } // same as nextvalue , but returns null after the timeout expires suspend fun nextvaluewithtimout t: long : int? { return withtimeoutornull t { nexvalue } } suspend fun longoperation { println starting ...</td>\n",
              "      <td>0.223055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1639854</th>\n",
              "      <td>\"https://github.com/chendaniely/chendaniely.github.io/issues/26\"</td>\n",
              "      <td>add a hobbies section</td>\n",
              "      <td>- photography - snowboarding - s.c.u.b.a - hobbes, the blue heeler mix :dog2:</td>\n",
              "      <td>0.234810</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                issue_url  ...      dist\n",
              "5016538                   \"https://github.com/SvanteAlnas/Test/issues/29\"  ...  0.217566\n",
              "465910           \"https://github.com/Kotlin/kotlinx.coroutines/issues/67\"  ...  0.223055\n",
              "1639854  \"https://github.com/chendaniely/chendaniely.github.io/issues/26\"  ...  0.234810\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECDm03VSFMPG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "outputId": "6500c5d7-5437-4604-973c-4ab2f1ae9cf3"
      },
      "source": [
        "seq2seq_inf_rec.demo_model_predictions(n=1, issue_df=testdf, threshold=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 5 =================\n",
            "\n",
            "\"https://github.com/firemodels/smv/issues/207\"\n",
            "Issue Body:\n",
            " the sphere used to distinguish hvac vents from regular vents is not always drawn with an appropriate size to large or too small depending on the size of the compartments being modled \n",
            "\n",
            "Original Title:\n",
            " problem with zone hvac vent display\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " feature request request a\n",
            "\n",
            "**** Similar Issues (using encoder embedding) ****:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_url</th>\n",
              "      <th>issue_title</th>\n",
              "      <th>body</th>\n",
              "      <th>dist</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1139719</th>\n",
              "      <td>\"https://github.com/beaugunderson/emoji-aware/issues/6\"</td>\n",
              "      <td>extract emoji characters</td>\n",
              "      <td>as discussed here https://twitter.com/notwaldorf/status/841773147296694272 , there might be a use case in this library to add an extract function to get an array of only the emoji characters.</td>\n",
              "      <td>0.352041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1905124</th>\n",
              "      <td>\"https://github.com/shadowsocks/ShadowsocksX-NG/issues/329\"</td>\n",
              "      <td>show qrcode for current server not working.</td>\n",
              "      <td>mac os x latest. shadowsocksx-ng latest. show qrcode for current server not working. can't decode the qrcode.</td>\n",
              "      <td>0.373637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4627902</th>\n",
              "      <td>\"https://github.com/OpenAngelArena/Bug-Reports/issues/27\"</td>\n",
              "      <td>spectre random tp after duel when haunt is active</td>\n",
              "      <td>as i got into duel as spectre i used haunt, we won the duel, my haunt was still active and i got tp somewhere random on the map mostly near the arena</td>\n",
              "      <td>0.378080</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                           issue_url  ...      dist\n",
              "1139719      \"https://github.com/beaugunderson/emoji-aware/issues/6\"  ...  0.352041\n",
              "1905124  \"https://github.com/shadowsocks/ShadowsocksX-NG/issues/329\"  ...  0.373637\n",
              "4627902    \"https://github.com/OpenAngelArena/Bug-Reports/issues/27\"  ...  0.378080\n",
              "\n",
              "[3 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSos9KtAFPf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c0bf5d8-18fd-4a01-c7e6-31546787e0ba"
      },
      "source": [
        "# incase you need to reset the rec system\n",
        "# seq2seq_inf_rec.set_recsys_annoyobj(recsys_annoyobj)\n",
        "# seq2seq_inf_rec.set_recsys_data(all_data_df)\n",
        "\n",
        "# save object\n",
        "recsys_annoyobj.save('recsys_annoyobj.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMVKXDnvFaXI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T5IzUgoFgKo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}